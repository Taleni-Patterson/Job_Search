{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Site dictionary\n",
    "site = \"http://au.indeed.com/jobs?q=TERM&l=Sydney+NSW&start=PAGE\"\n",
    "page_iterator =  10\n",
    "domain = \"http://au.indeed.com\"\n",
    "term = \"analyst\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished collecting the list of job descriptions.\n",
      "Finished Downloading: 854 out of 854: 99.883000 [#########]\r"
     ]
    }
   ],
   "source": [
    "page = 1\n",
    "old_list_of_page_links, new_list_of_page_links, list_of_page_links = [], [], []\n",
    "failed = []\n",
    "noNew = False\n",
    "job_desc=[]\n",
    "\n",
    "while noNew == False:\n",
    "\n",
    "    # Collecting a list of pages to scrape and saving their job titles and links\n",
    "    result_number = page*page_iterator\n",
    "    req = requests.get(site.replace('PAGE', str(result_number)))\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    div = soup.find_all('div', {'class':'title'})\n",
    "    new_list_of_page_links = [domain + d.a['href'] for d in div]\n",
    "    \n",
    "    # Check whether there were any new\n",
    "    if page == 50:\n",
    "        noNew = True\n",
    "    else:\n",
    "        list_of_page_links +=[domain + d.a['href'] for d in div]\n",
    "        old_list_of_page_links = new_list_of_page_links\n",
    "    \n",
    "        # Go to the next page of search results\n",
    "        page += 1\n",
    "\n",
    "print('Finished collecting the list of job descriptions.')\n",
    "\n",
    "# Crawling through each page\n",
    "for i,p in enumerate(list_of_page_links):\n",
    "    try:\n",
    "        req = requests.get(p)\n",
    "        soup = BeautifulSoup(req.text,'html.parser')\n",
    "        main_text = soup.find('div', {'id':'jobDescriptionText'})\n",
    "        job_desc.append(main_text)\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        failed.append(p)\n",
    "    print('Finished Downloading: %i out of %i: %f [%s]'% (i+1, len(list_of_page_links), round(i/len(list_of_page_links)*100,3),'#'*int(i/len(list_of_page_links)*10)+' '*(1-int(len(list_of_page_links)-i/len(list_of_page_links))*10)),end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"jobsearch-jobDescriptionText\" id=\"jobDescriptionText\"><p><b>Customer Service Assistant - Food Industry</b></p><p><b>spiralfoods.com.au</b></p><p>Spiral Foods is a leading Australian organic and natural foods company, with iconic brand such as BONSOY.</p><p>We’re looking for an experienced customer service staff member for a permanent position in our Coburg office.</p><p><b>We are looking for someone who is: </b></p><ul><li>Approachable, friendly, reliable, with a can-do attitude.</li><li>Comfortable with conducting customer communications, either face to face, phone or via email in a friendly and professional manner</li><li>Super organised and be able to work towards deadlines</li><li>Attention to details</li><li>Numerate and proficient with MS Office</li><li>Food lovers ideal but not required</li><li>SAP experience or equivalent preferred</li></ul><p><b>Key responsibilities of Customer Service Assistant: </b></p><ul><li>Taking inbound queries through phone, email &amp; web</li><li>Fulfilling orders and invoicing accordingly</li><li>Manage daily data entry relating to the deliveries/stock movements/cash receipts</li></ul><p><b>What’s on offer: </b></p><ul><li>Part of a very successful, growing Australian owned business</li><li>Long term position with room to grow in a small and supportive working environment</li><li>Training for all aspects of your position will be provided</li></ul><p><b>You will need to be: </b></p><ul><li>Australian citizen or permanent resident</li><li>Minimum 3-5 years customer service experiences</li></ul><p>Job Types: Full-time, Permanent</p><p>Experience:</p><ul><li>inbound call centre: 1 year (Preferred)</li><li>customer service: 3 years (Preferred)</li></ul><p>Microsoft Office Experience:</p><ul><li>Excel (Preferred)</li></ul><p>Benefits:</p><ul><li>Staff / corporate discounts</li></ul><p>Job Duties:</p><ul><li>Answer incoming customer inquiries</li><li>Record and modify customer information within the database</li><li>Engage with clients in a friendly and professional manner while actively listening to their concerns</li><li>Offer support and solutions to customers in accordance with the company's customer service policies</li><li>Collaborate with key stakeholders and teams to stay updated on new products, services, and policies</li></ul></div>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_desc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "r = str(job_desc[0])\n",
    "titles = re.findall('<b>(.+?)</b>', r)\n",
    "content = re.findall('</b></p><p>(.+?)</p>', r)\n",
    "d = {t:c for t,c in zip(titles, content)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<b>spiralfoods.com.au</b>']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Jora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Glassdoor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in d.keys():\n",
    "    if s in site:\n",
    "        page = 0\n",
    "        site_dict = d[s]\n",
    "        site_dict[\"TERM\"] = term\n",
    "        noNew = False\n",
    "        old_list_of_page_links = []\n",
    "        job_desc  = []\n",
    "        \n",
    "        while noNew == False:\n",
    "        # Setting the required variables.\n",
    "            site_dict[\"PAGE\"] = str(page*site_dict[\"PAGE_ITERATOR\"])\n",
    "\n",
    "            # Loading the page\n",
    "            pattern = re.compile(\"|\".join(site_dict[\"replacements\"]))\n",
    "            site_search = pattern.sub(lambda m: site_dict[re.escape(m.group(0))],site_dict[\"site\"])\n",
    "            req = requests.get(site_search)\n",
    "            \n",
    "            # Collecting a list of pages to scrape and saving their job titles and links\n",
    "            soup = BeautifulSoup(req.text, 'html.parser')\n",
    "            div = soup.find_all('div', {'class':'title'})\n",
    "            list_of_page_links = [site_dict[\"DOMAIN\"] + d.a['href'] for d in div]\n",
    "            \n",
    "            # Check whether there were any new\n",
    "            if list_of_page_links == old_list_of_page_links:\n",
    "                noNew = True\n",
    "\n",
    "            # Crawling through each page\n",
    "            for p in list_of_page_links:\n",
    "                req = requests.get(p)\n",
    "                soup = BeautifulSoup(req.text,'html.parser')\n",
    "                main_text = soup.find('div', {'id':'jobDescriptionText'})\n",
    "                job_desc.append(main_text)\n",
    "\n",
    "            # Go to the next page of search results\n",
    "            page += 1\n",
    "            \n",
    "            # Setting up old list of page links\n",
    "            old_list_of_page_links = list_of_page_links"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
